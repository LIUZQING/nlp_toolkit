{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo tutorial for how to use nlp_toolkit to train sequence labeling model and predict new samples. The task we choose is chunking labeling.\n",
    "\n",
    "The dataset includes working experience texts from different cv, and we want to label noun phrases in given text.\n",
    "\n",
    "Available models:\n",
    "\n",
    "1. WordRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/wangyilei/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from nlp_toolkit import Dataset, Labeler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "config = yaml.load(open('../nlp_toolkit/config.yaml'))\n",
    "config['data']['inner_char'] = True\n",
    "config['embed']['pre'] = False\n",
    "config['train']['metric'] = 'f1_seq'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 21:21:35,567 - data.py[line:89] - INFO: data loaded\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(fname='../data/cv_word.txt', task_type='sequence_labeling', mode='train', segment=False, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主要 帮助 工地 师傅 一起 超平 , 防线 工作 O O B-Chunk E-Chunk O O O O O\n",
      "协助 线 上 、 线 下 活动 的 执行 O O O O B-Chunk I-Chunk E-Chunk O O\n",
      "执行 各项 培训 相关 的 各项 工作 流程 O O O O O O B-Chunk E-Chunk\n",
      "云南 : 曲靖 、 昭通 下属 的 5 个 县级 供电 公司 10 个 供电所 O O O O O O O O O O B-Chunk E-Chunk O O O\n",
      "担任 培训 学校 英语 讲师 一 职 和 学生 管理 O O B-Chunk I-Chunk E-Chunk O O O B-Chunk E-Chunk\n",
      "搜寻 招标 公告 , 告知 领导 及 业务 人员 , 确认 是否 报名 O B-Chunk E-Chunk O O O O B-Chunk E-Chunk O O O O\n",
      "2001 / 10 -- 2002 / 04 : 上海 润 宝 工贸 公司 <s> 所属 行业 : <s> 环保 <s> 销售部 <s> 销售 代表 <s> 负责 江浙 一带 工业 圆 区 的 空气过滤器 的 销售 和 维护 , 期间 昆山 翊 腾 电子 是 长期 的 客户 O O O O O O O O O O O B-Chunk E-Chunk O O O O O O O O O B-Chunk E-Chunk O O O O O O O O O O O O O O O O O O O O O O O\n",
      "<s> 仓库 管理 : 对 仓库 进行 合理 布局 , 为 方便 员工 操作 和 减少 失误 , 能够 独立 编排 库 位 图 和 货位 表 O B-Chunk E-Chunk O O O O O O O O O B-Chunk E-Chunk O O O O O O O O O O O O O\n",
      "<s> 档案 管理 : 能 独立 制定 仓库 管理 文档 , 专人 负责 仓库 资料 的 更新 归档 并 定期 检查 O B-Chunk E-Chunk O O O O B-Chunk E-Chunk O O O O O O O O O O O O\n",
      "电子 技术 / 半导体 / 集成电路 B-Chunk E-Chunk O O O O\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(dataset.texts[0:10], dataset.labels[0:10]):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 21:21:55,130 - data.py[line:116] - INFO: texts and labels transformed to number index\n",
      "2018-11-10 21:21:55,131 - data.py[line:124] - INFO: Use Embeddings from Straching\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57415, 100) (57415, 100, 4)\n"
     ]
    }
   ],
   "source": [
    "# if we want to use pre_trained embeddings, we need a gensim-format embedding file\n",
    "x, y, config = dataset.transform()\n",
    "print(x['word'].shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if your want to see the vocab and label index mapping dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.transformer._word_vocab._token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.transformer._label_vocab._token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = dataset.transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Sequence Labeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='word_rnn'\n",
    "seq_labeler = Labeler(model_name=model_name, transformer=transformer, seq_type='bucket', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 20:47:03,102 - trainer.py[line:113] - INFO: word_rnn model structure...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char (InputLayer)               (None, None, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, None, 3 114016      char[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "word (InputLayer)               (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_cnn (TimeDistributed)      (None, None, None, 6 4160        time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, None, 64)     2539072     word[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "char_pooling (TimeDistributed)  (None, None, 64)     0           char_cnn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 64)     4160        word_embedding[0][0]             \n",
      "                                                                 char_pooling[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, 64)     0           dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 64)     4160        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 64)     0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, None, 64)     0           lambda_1[0][0]                   \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, None, 64)     0           dense_2[0][0]                    \n",
      "                                                                 word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, None, 64)     0           subtract_1[0][0]                 \n",
      "                                                                 char_pooling[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_feature (Add)         (None, None, 64)     0           multiply_1[0][0]                 \n",
      "                                                                 multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "word_lstm_1 (Bidirectional)     (None, None, 256)    197632      attention_feature[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "word_lstm_2 (Bidirectional)     (None, None, 256)    394240      word_lstm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, None, 4)      1052        word_lstm_2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 3,258,492\n",
      "Trainable params: 3,258,492\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 20:47:03,549 - trainer.py[line:123] - INFO: train/valid set: 45932/11483\n",
      "2018-11-10 20:47:03,550 - trainer.py[line:80] - INFO: use bucket sequence to speed up model training\n",
      "2018-11-10 20:47:03,552 - sequence.py[line:300] - INFO: Training with 77 non-empty buckets\n",
      "2018-11-10 20:47:03,898 - sequence.py[line:300] - INFO: Training with 77 non-empty buckets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mointor training process using f1 score and label acc\n",
      "Successfully made a directory: models/word_rnn_201811102046\n",
      "using Early Stopping\n",
      "using Reduce LR On Plateau\n",
      "tracking loss history and metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 20:47:04,855 - trainer.py[line:154] - INFO: saving model parameters and transformer to models/word_rnn_201811102046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model hyperparameters:\n",
      " {'nb_classes': 4, 'nb_tokens': 39673, 'maxlen': None, 'embedding_dim': 64, 'rnn_type': 'lstm', 'nb_rnn_layers': 2, 'drop_rate': 0.5, 're_drop_rate': 0.15, 'use_crf': True, 'inner_char': True, 'word_rnn_size': 128, 'integration_method': 'attention', 'char_feature_method': 'cnn', 'max_charlen': 10, 'nb_char_tokens': 3563, 'char_embedding_dim': 32, 'nb_filters': 64, 'conv_kernel_size': 2}\n",
      "Epoch 1/25\n",
      "756/756 [==============================] - 165s 218ms/step - loss: 0.2883 - acc: 0.8770 - val_loss: 0.1398 - val_acc: 0.9258\n",
      " - acc: 84.46\n",
      " - f1: 87.70\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      Chunk       0.91      0.84      0.88      1523\n",
      "\n",
      "avg / total       0.91      0.84      0.88      1523\n",
      "\n",
      "\n",
      "Epoch 00001: f1_seq improved from -inf to 0.87705, saving model to models/word_rnn_201811102046/model_weights_01_0.9258_0.8770.h5\n",
      "Epoch 2/25\n",
      "756/756 [==============================] - 156s 206ms/step - loss: 0.1130 - acc: 0.9330 - val_loss: 0.1061 - val_acc: 0.9273\n",
      " - acc: 86.62\n",
      " - f1: 89.38\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      Chunk       0.92      0.87      0.89      1520\n",
      "\n",
      "avg / total       0.92      0.87      0.89      1520\n",
      "\n",
      "\n",
      "Epoch 00002: f1_seq improved from 0.87705 to 0.89376, saving model to models/word_rnn_201811102046/model_weights_02_0.9273_0.8938.h5\n",
      "Epoch 3/25\n",
      "756/756 [==============================] - 154s 204ms/step - loss: 0.0735 - acc: 0.9437 - val_loss: 0.0864 - val_acc: 0.9332\n",
      " - acc: 86.70\n",
      " - f1: 89.14\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      Chunk       0.92      0.86      0.89      1519\n",
      "\n",
      "avg / total       0.92      0.86      0.89      1519\n",
      "\n",
      "\n",
      "Epoch 00003: f1_seq did not improve from 0.89376\n",
      "Epoch 4/25\n",
      "756/756 [==============================] - 152s 202ms/step - loss: 0.0654 - acc: 0.9459 - val_loss: 0.0842 - val_acc: 0.9339\n",
      " - acc: 86.43\n",
      " - f1: 88.96\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      Chunk       0.92      0.86      0.89      1520\n",
      "\n",
      "avg / total       0.92      0.86      0.89      1520\n",
      "\n",
      "\n",
      "Epoch 00004: f1_seq did not improve from 0.89376\n",
      "Epoch 5/25\n",
      "756/756 [==============================] - 157s 208ms/step - loss: 0.0633 - acc: 0.9463 - val_loss: 0.0838 - val_acc: 0.9340\n",
      " - acc: 86.41\n",
      " - f1: 89.01\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      Chunk       0.92      0.86      0.89      1527\n",
      "\n",
      "avg / total       0.92      0.86      0.89      1527\n",
      "\n",
      "\n",
      "Epoch 00005: f1_seq did not improve from 0.89376\n",
      "best f1_seq: 0.89\n"
     ]
    }
   ],
   "source": [
    "trained_model = seq_labeler.train(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 21:21:56,592 - trainer.py[line:172] - INFO: 10-fold starts!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------ fold 0------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 21:21:58,183 - trainer.py[line:183] - INFO: word_rnn model structure...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char (InputLayer)               (None, None, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, None, 3 114016      char[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "word (InputLayer)               (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_cnn (TimeDistributed)      (None, None, None, 6 4160        time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, None, 64)     2539072     word[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "char_pooling (TimeDistributed)  (None, None, 64)     0           char_cnn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 64)     4160        word_embedding[0][0]             \n",
      "                                                                 char_pooling[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, 64)     0           dense_3[0][0]                    \n",
      "                                                                 dense_3[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, None, 64)     4160        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 64)     0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, None, 64)     0           lambda_1[0][0]                   \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, None, 64)     0           dense_4[0][0]                    \n",
      "                                                                 word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, None, 64)     0           subtract_1[0][0]                 \n",
      "                                                                 char_pooling[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_feature (Add)         (None, None, 64)     0           multiply_1[0][0]                 \n",
      "                                                                 multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "word_lstm_1 (Bidirectional)     (None, None, 256)    197632      attention_feature[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "word_lstm_2 (Bidirectional)     (None, None, 256)    394240      word_lstm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, None, 4)      1052        word_lstm_2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 3,258,492\n",
      "Trainable params: 3,258,492\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 21:22:01,797 - trainer.py[line:83] - INFO: use bucket sequence to speed up model training\n",
      "2018-11-10 21:22:01,799 - sequence.py[line:300] - INFO: Training with 77 non-empty buckets\n",
      "2018-11-10 21:22:02,207 - sequence.py[line:300] - INFO: Training with 77 non-empty buckets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mointor training process using f1 score and label acc\n",
      "using Early Stopping\n",
      "using Reduce LR On Plateau\n",
      "tracking loss history and metrics\n",
      "Epoch 1/25\n",
      " 20/840 [..............................] - ETA: 7:44 - loss: 0.9048 - acc: 0.7473"
     ]
    }
   ],
   "source": [
    "config['train']['train_mode'] = 'fold'\n",
    "seq_labeler = Labeler(model_name=model_name, transformer=transformer, config=config)\n",
    "seq_labeler.train(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict New Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 21:10:46,455 - data.py[line:73] - INFO: transformer loaded\n",
      "2018-11-10 21:10:46,786 - data.py[line:89] - INFO: data loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data transformer loaded\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset('../data/cv_word_predict.txt',\n",
    "                  task_type='sequence_labeling', mode='predict',\n",
    "                  tran_fname='models/word_rnn_201811102046/transformer.h5',\n",
    "                  segment=False)\n",
    "x_seq = dataset.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "seq_labeler = Labeler('word_rnn', dataset.transformer)\n",
    "seq_labeler.load(weight_fname='models/word_rnn_201811102046/model_weights_02_0.9273_0.8938.h5',\n",
    "                 para_fname='models/word_rnn_201811102046/model_parameters.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 21:13:12,591 - labeler.py[line:119] - INFO: predict 57415 samples used 131.9s\n"
     ]
    }
   ],
   "source": [
    "y_pred = seq_labeler.predict(x_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('主要', 'O'), ('帮助', 'O'), ('工地', 'B-Chunk'), ('师傅', 'E-Chunk'), ('一起', 'O'), ('超平', 'O'), (',', 'O'), ('防线', 'O'), ('工作', 'O')]\n"
     ]
    }
   ],
   "source": [
    "x_len = x_seq['length']\n",
    "y_pred_true = [y_pred[i][:x_len[i]] for i in range(len(x_len))]\n",
    "print([(x, y) for x, y in zip(dataset.texts[0].split(' '), y_pred_true[0])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
