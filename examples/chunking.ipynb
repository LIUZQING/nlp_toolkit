{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo tutorial for how to use nlp_toolkit to train sequence labeling model and predict new samples. The task we choose is noun phrases labeling.\n",
    "\n",
    "The dataset includes working experience texts from different cv, and we want to label noun phrases in given text.\n",
    "\n",
    "Available models:\n",
    "\n",
    "1. WordRNN\n",
    "2. CharRNN\n",
    "3. IDCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/wangyilei/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from nlp_toolkit import Dataset, Labeler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "config = yaml.load(open('../nlp_toolkit/config_sequence_labeling.yaml'))\n",
    "config['data']['inner_char'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:43:35,198 - data.py[line:130] - INFO: data loaded\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(fname='../data/cv_word.txt', task_type='sequence_labeling', mode='train', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('主要', 'O'), ('帮助', 'O'), ('工地', 'B-Chunk'), ('师傅', 'E-Chunk'), ('一起', 'O'), ('超平', 'O'), (',', 'O'), ('防线', 'O'), ('工作', 'O')]\n",
      "[('协助', 'O'), ('线', 'O'), ('上', 'O'), ('、', 'O'), ('线', 'B-Chunk'), ('下', 'I-Chunk'), ('活动', 'E-Chunk'), ('的', 'O'), ('执行', 'O')]\n",
      "[('执行', 'O'), ('各项', 'O'), ('培训', 'O'), ('相关', 'O'), ('的', 'O'), ('各项', 'O'), ('工作', 'B-Chunk'), ('流程', 'E-Chunk')]\n",
      "[('云南', 'O'), (':', 'O'), ('曲靖', 'O'), ('、', 'O'), ('昭通', 'O'), ('下属', 'O'), ('的', 'O'), ('5', 'O'), ('个', 'O'), ('县级', 'O'), ('供电', 'B-Chunk'), ('公司', 'E-Chunk'), ('10', 'O'), ('个', 'O'), ('供电所', 'O')]\n",
      "[('担任', 'O'), ('培训', 'O'), ('学校', 'B-Chunk'), ('英语', 'I-Chunk'), ('讲师', 'E-Chunk'), ('一', 'O'), ('职', 'O'), ('和', 'O'), ('学生', 'B-Chunk'), ('管理', 'E-Chunk')]\n",
      "[('搜寻', 'O'), ('招标', 'B-Chunk'), ('公告', 'E-Chunk'), (',', 'O'), ('告知', 'O'), ('领导', 'O'), ('及', 'O'), ('业务', 'B-Chunk'), ('人员', 'E-Chunk'), (',', 'O'), ('确认', 'O'), ('是否', 'O'), ('报名', 'O')]\n",
      "[('2001', 'O'), ('/', 'O'), ('10', 'O'), ('--', 'O'), ('2002', 'O'), ('/', 'O'), ('04', 'O'), (':', 'O'), ('上海', 'O'), ('润', 'O'), ('宝', 'O'), ('工贸', 'B-Chunk'), ('公司', 'E-Chunk'), ('<s>', 'O'), ('所属', 'O'), ('行业', 'O'), (':', 'O'), ('<s>', 'O'), ('环保', 'O'), ('<s>', 'O'), ('销售部', 'O'), ('<s>', 'O'), ('销售', 'B-Chunk'), ('代表', 'E-Chunk'), ('<s>', 'O'), ('负责', 'O'), ('江浙', 'O'), ('一带', 'O'), ('工业', 'O'), ('圆', 'O'), ('区', 'O'), ('的', 'O'), ('空气过滤器', 'O'), ('的', 'O'), ('销售', 'O'), ('和', 'O'), ('维护', 'O'), (',', 'O'), ('期间', 'O'), ('昆山', 'O'), ('翊', 'O'), ('腾', 'O'), ('电子', 'O'), ('是', 'O'), ('长期', 'O'), ('的', 'O'), ('客户', 'O')]\n",
      "[('<s>', 'O'), ('仓库', 'B-Chunk'), ('管理', 'E-Chunk'), (':', 'O'), ('对', 'O'), ('仓库', 'O'), ('进行', 'O'), ('合理', 'O'), ('布局', 'O'), (',', 'O'), ('为', 'O'), ('方便', 'O'), ('员工', 'B-Chunk'), ('操作', 'E-Chunk'), ('和', 'O'), ('减少', 'O'), ('失误', 'O'), (',', 'O'), ('能够', 'O'), ('独立', 'O'), ('编排', 'O'), ('库', 'O'), ('位', 'O'), ('图', 'O'), ('和', 'O'), ('货位', 'O'), ('表', 'O')]\n",
      "[('<s>', 'O'), ('档案', 'B-Chunk'), ('管理', 'E-Chunk'), (':', 'O'), ('能', 'O'), ('独立', 'O'), ('制定', 'O'), ('仓库', 'B-Chunk'), ('管理', 'E-Chunk'), ('文档', 'O'), (',', 'O'), ('专人', 'O'), ('负责', 'O'), ('仓库', 'O'), ('资料', 'O'), ('的', 'O'), ('更新', 'O'), ('归档', 'O'), ('并', 'O'), ('定期', 'O'), ('检查', 'O')]\n",
      "[('电子', 'B-Chunk'), ('技术', 'E-Chunk'), ('/', 'O'), ('半导体', 'O'), ('/', 'O'), ('集成电路', 'O')]\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(dataset.texts[0:10], dataset.labels[0:10]):\n",
    "    print(list(zip(x, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:43:47,123 - data.py[line:215] - INFO: texts and labels have been transformed to number index\n",
      "2018-11-18 23:43:47,124 - data.py[line:223] - INFO: Use Embeddings from Straching\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57415, 80) (57415, 80, 4)\n"
     ]
    }
   ],
   "source": [
    "# if we want to use pre_trained embeddings, we need a gensim-format embedding file\n",
    "x, y, config = dataset.transform()\n",
    "print(x['token'].shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if your want to see the vocab and label index mapping dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.transformer._word_vocab._token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.transformer._label_vocab._token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = dataset.transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Sequence Labeler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avialiable models: model_name_list = ['word_rnn', 'char_rnn', 'idcnn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='word_rnn'\n",
    "seq_labeler = Labeler(model_name=model_name, transformer=transformer, seq_type='bucket', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:43:50,095 - trainer.py[line:129] - INFO: word_rnn model structure...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char (InputLayer)               (None, None, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token (InputLayer)              (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, None, 3 114016      char[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, None, 64)     2539072     token[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "char_lstm (TimeDistributed)     (None, None, 64)     16640       time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 64)     4160        word_embedding[0][0]             \n",
      "                                                                 char_lstm[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, 64)     0           dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 64)     4160        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 64)     0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, None, 64)     0           lambda_1[0][0]                   \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, None, 64)     0           dense_2[0][0]                    \n",
      "                                                                 word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, None, 64)     0           subtract_1[0][0]                 \n",
      "                                                                 char_lstm[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_feature (Add)         (None, None, 64)     0           multiply_1[0][0]                 \n",
      "                                                                 multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "word_lstm_1 (Bidirectional)     (None, None, 256)    197632      attention_feature[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "word_lstm_2 (Bidirectional)     (None, None, 256)    394240      word_lstm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, None, 4)      1052        word_lstm_2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 3,270,972\n",
      "Trainable params: 3,270,972\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:43:50,848 - trainer.py[line:139] - INFO: train/valid set: 45932/11483\n",
      "2018-11-18 23:43:50,849 - trainer.py[line:89] - INFO: use bucket sequence to speed up model training\n",
      "2018-11-18 23:43:50,851 - sequence.py[line:346] - INFO: Training with 77 non-empty buckets\n",
      "2018-11-18 23:43:51,206 - sequence.py[line:346] - INFO: Training with 77 non-empty buckets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mointor training process using f1 score and label acc\n",
      "Successfully made a directory: models/word_rnn_201811182343\n",
      "using Early Stopping\n",
      "using Reduce LR On Plateau\n",
      "tracking loss history and metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-18 23:43:52,132 - trainer.py[line:170] - INFO: saving model parameters and transformer to models/word_rnn_201811182343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model hyperparameters:\n",
      " {'nb_classes': 4, 'nb_tokens': 39673, 'maxlen': None, 'embedding_dim': 64, 'rnn_type': 'lstm', 'nb_rnn_layers': 2, 'drop_rate': 0.5, 're_drop_rate': 0.15, 'use_crf': True, 'inner_char': True, 'word_rnn_size': 128, 'integration_method': 'attention', 'char_feature_method': 'rnn', 'max_charlen': 10, 'nb_char_tokens': 3563, 'char_embedding_dim': 32, 'char_rnn_size': 32}\n",
      "Epoch 1/25\n",
      "535/753 [====================>.........] - ETA: 48s - loss: 0.4368 - acc: 0.8540"
     ]
    }
   ],
   "source": [
    "trained_model = seq_labeler.train(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['train']['train_mode'] = 'fold'\n",
    "seq_labeler_new = Labeler(model_name=model_name, transformer=transformer, seq_type='bucket', config=config)\n",
    "seq_labeler_new.train(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict New Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(fname='../data/cv_word_predict.txt',\n",
    "                  task_type='sequence_labeling', mode='predict',\n",
    "                  tran_fname='models/word_rnn_201811102046/transformer.h5')\n",
    "x_seq = dataset.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_labeler = Labeler('word_rnn', dataset.transformer)\n",
    "seq_labeler.load(weight_fname='models/word_rnn_201811102046/model_weights_02_0.9273_0.8938.h5',\n",
    "                 para_fname='models/word_rnn_201811102046/model_parameters.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = seq_labeler.predict(x_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len = x_seq['length']\n",
    "y_pred_true = [y_pred[i][:x_len[i]] for i in range(len(x_len))]\n",
    "print([(x, y) for x, y in zip(dataset.texts[0].split(' '), y_pred_true[0])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
