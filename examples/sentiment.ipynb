{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimet Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo tutorial for how to use nlp_toolkit to train classification model and predict new samples. The task we choose is sentiment binary classification.\n",
    "\n",
    "The dataset is crawled from Kanzhun.com and Dajie.com, which is about company pros and cons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available models:\n",
    "    1. Bi-LSTM Attention\n",
    "    2. Multi Head Self Attention\n",
    "    3. TextCNN\n",
    "    4. DPCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from nlp_toolkit import Dataset, Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "config = yaml.load(open('../config_classification.yaml', encoding='utf8'))\n",
    "#config['data']['basic_token'] = 'char'\n",
    "#config['embed']['pre'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-24 23:27:51,934 - data.py[line:136] - INFO: data loaded\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(fname='../data/company_pro_con.txt', task_type='classification', mode='train', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "进去 前 许诺 的 工资 给 的 高 0\n",
      "校园 环境 优美 ， 美女 很多 ， 适合 居住 ， 食堂 饭菜 便宜 ， 操场 好 ， 可以 天天 运动 0\n",
      "老板 人 很好 老 员工 会 各种 教 你 东西 ， 而且 不会 有所 保留 薪水 在 大连 还 算 可以 0\n",
      "人员 比较 多 ， 复杂   办公室 容易 形成 拉帮结派 不利于 企业 发展 1\n",
      "出差 太多 了 。 在 现场 开发 很苦 逼 。 1\n",
      "公司 目前 地理位置 不 太 理想 ， 离 城市 中心 较 远点 。 1\n",
      "公司 的 技术 水平 国内 顶尖 ， 十几 年 的 资历 ， 制作 的 作品 几乎 都 是 精品 ， 参与 过 很多 知名 项目 。 0\n",
      "工作 流程 复杂     个人 上升 空间 有限     新产品 的 创新 能力 有限   组织 架构 稍 显 臃肿 1\n",
      "无偿 加班 ， 加班 多 ， 没 加班费 ， 压力 很大 1\n",
      "环境 比较 轻松 ， 跟 项目 走 ， 能 学 不少 专业 知识 ， 经验 很 重要 0\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(dataset.texts[0:10], dataset.labels[0:10]):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-24 23:28:00,514 - data.py[line:214] - INFO: texts and labels have been transformed to number index\n",
      "2018-11-24 23:28:05,272 - utilities.py[line:226] - INFO: OOV rate: 0.00 %\n",
      "2018-11-24 23:28:05,288 - data.py[line:220] - INFO: Loaded Pre_trained Embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94635, 100) (94635, 2)\n"
     ]
    }
   ],
   "source": [
    "# if we want to use pre_trained embeddings, we need a gensim-format embedding file\n",
    "x, y, config = dataset.transform()\n",
    "print(x['token'].shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if your want to see the vocab and label index mapping dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.transformer._word_vocab._token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.transformer._label_vocab._token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = dataset.transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avialiable models: model_name_list = ['dpcnn', 'text_cnn', 'bi_lstm_att', 'multi_head_self_att']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='bi_lstm_att'\n",
    "# if you want to get attention weights during prediction, please set return_attention=True, only for bi_lstm_att model\n",
    "config['model'][model_name]['return_att'] = True\n",
    "# text_cnn and dpcnn currently not support bucket sequence type\n",
    "text_classifier = Classifier(model_name=model_name, transformer=transformer, seq_type='bucket', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-24 23:28:21,606 - trainer.py[line:129] - INFO: bi_lstm_att model structure...\n",
      "2018-11-24 23:28:21,632 - trainer.py[line:139] - INFO: train/valid set: 75708/18927\n",
      "2018-11-24 23:28:21,633 - trainer.py[line:89] - INFO: use bucket sequence to speed up model training\n",
      "2018-11-24 23:28:21,635 - sequence.py[line:346] - INFO: Training with 99 non-empty buckets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "token (InputLayer)              (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_embeddings (Embedding)    (None, None, 300)    7333200     token[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, None, 300)    0           token_embeddings[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 300)    0           spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_0 (Bidirectional)       (None, None, 1024)   3330048     activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_1 (Bidirectional)       (None, None, 1024)   6295552     bi_lstm_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 2348)   0           bi_lstm_1[0][0]                  \n",
      "                                                                 bi_lstm_0[0][0]                  \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attlayer (Attention)            [(None, 2348), (None 300800      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2348)         0           attlayer[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Dense)                 (None, 2)            4698        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Concatenate)           (None, None)         0           softmax[0][0]                    \n",
      "                                                                 attlayer[0][1]                   \n",
      "==================================================================================================\n",
      "Total params: 17,264,298\n",
      "Trainable params: 17,264,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-24 23:28:21,895 - sequence.py[line:346] - INFO: Training with 99 non-empty buckets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mointor training process using f1 score\n",
      "Successfully made a directory: models\\bi_lstm_att_201811242328\n",
      "using Early Stopping\n",
      "using Reduce LR On Plateau\n",
      "tracking loss history and metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-24 23:28:22,211 - trainer.py[line:171] - INFO: saving model parameters and transformer to models\\bi_lstm_att_201811242328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model hyperparameters:\n",
      " {'nb_classes': 2, 'nb_tokens': 24444, 'maxlen': None, 'embedding_dim': 300, 'rnn_size': 512, 'attention_dim': 128, 'embed_dropout_rate': 0.15, 'final_dropout_rate': 0.5, 'return_attention': True}\n",
      "Epoch 1/25\n",
      "1230/1230 [==============================] - 125s 102ms/step - loss: 0.4283 - acc: 0.7401 - val_loss: 0.2717 - val_acc: 0.8130\n",
      "0 - f1: 92.99\n",
      "1 - f1: 91.98\n",
      "\n",
      "Epoch 00001: f1 improved from -inf to 0.92509, saving model to models\\bi_lstm_att_201811242328\\model_weights_01_0.8130_0.9251.h5\n",
      "Epoch 2/25\n",
      "1230/1230 [==============================] - 120s 97ms/step - loss: 0.2227 - acc: 0.8430 - val_loss: 0.2179 - val_acc: 0.8461\n",
      "0 - f1: 94.42\n",
      "1 - f1: 93.85\n",
      "\n",
      "Epoch 00002: f1 improved from 0.92509 to 0.94149, saving model to models\\bi_lstm_att_201811242328\\model_weights_02_0.8461_0.9415.h5\n",
      "Epoch 3/25\n",
      "1230/1230 [==============================] - 120s 97ms/step - loss: 0.1648 - acc: 0.8721 - val_loss: 0.2083 - val_acc: 0.8514\n",
      "0 - f1: 94.55\n",
      "1 - f1: 94.04\n",
      "\n",
      "Epoch 00003: f1 improved from 0.94149 to 0.94305, saving model to models\\bi_lstm_att_201811242328\\model_weights_03_0.8514_0.9431.h5\n",
      "Epoch 4/25\n",
      "1230/1230 [==============================] - 120s 97ms/step - loss: 0.1481 - acc: 0.8775 - val_loss: 0.2143 - val_acc: 0.8401\n",
      "0 - f1: 94.53\n",
      "1 - f1: 94.00\n",
      "\n",
      "Epoch 00004: f1 did not improve from 0.94305\n",
      "Epoch 5/25\n",
      "1230/1230 [==============================] - 119s 97ms/step - loss: 0.1423 - acc: 0.8717 - val_loss: 0.2162 - val_acc: 0.8395\n",
      "0 - f1: 94.53\n",
      "1 - f1: 94.02\n",
      "\n",
      "Epoch 00005: f1 did not improve from 0.94305\n",
      "Epoch 6/25\n",
      "1230/1230 [==============================] - 120s 98ms/step - loss: 0.1418 - acc: 0.8681 - val_loss: 0.2177 - val_acc: 0.8386\n",
      "0 - f1: 94.57\n",
      "1 - f1: 94.05\n",
      "\n",
      "Epoch 00006: f1 improved from 0.94305 to 0.94320, saving model to models\\bi_lstm_att_201811242328\\model_weights_06_0.8386_0.9432.h5\n",
      "Epoch 7/25\n",
      "1230/1230 [==============================] - 119s 97ms/step - loss: 0.1404 - acc: 0.8684 - val_loss: 0.2176 - val_acc: 0.8364\n",
      "0 - f1: 94.61\n",
      "1 - f1: 94.08\n",
      "\n",
      "Epoch 00007: f1 improved from 0.94320 to 0.94359, saving model to models\\bi_lstm_att_201811242328\\model_weights_07_0.8364_0.9436.h5\n",
      "Epoch 8/25\n",
      "1230/1230 [==============================] - 120s 97ms/step - loss: 0.1380 - acc: 0.8647 - val_loss: 0.2180 - val_acc: 0.8366\n",
      "0 - f1: 94.56\n",
      "1 - f1: 94.04\n",
      "\n",
      "Epoch 00008: f1 did not improve from 0.94359\n",
      "Epoch 9/25\n",
      "1230/1230 [==============================] - 120s 97ms/step - loss: 0.1368 - acc: 0.8642 - val_loss: 0.2190 - val_acc: 0.8334\n",
      "0 - f1: 94.56\n",
      "1 - f1: 94.02\n",
      "\n",
      "Epoch 00009: f1 did not improve from 0.94359\n",
      "Epoch 10/25\n",
      "1230/1230 [==============================] - 120s 97ms/step - loss: 0.1356 - acc: 0.8603 - val_loss: 0.2189 - val_acc: 0.8299\n",
      "0 - f1: 94.57\n",
      "1 - f1: 94.02\n",
      "\n",
      "Epoch 00010: f1 did not improve from 0.94359\n",
      "best f1: 94.36\n"
     ]
    }
   ],
   "source": [
    "trained_model = text_classifier.train(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 10:22:05,602 - trainer.py[line:186] - INFO: 10-fold starts!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------ fold 0------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 10:22:08,931 - trainer.py[line:199] - INFO: bi_lstm_att model structure...\n",
      "2018-11-25 10:22:08,948 - trainer.py[line:89] - INFO: use bucket sequence to speed up model training\n",
      "2018-11-25 10:22:08,950 - sequence.py[line:346] - INFO: Training with 99 non-empty buckets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "token (InputLayer)              (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_embeddings (Embedding)    (None, None, 300)    7333200     token[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_3 (SpatialDro (None, None, 300)    0           token_embeddings[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, 300)    0           spatial_dropout1d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_0 (Bidirectional)       (None, None, 1024)   3330048     activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_1 (Bidirectional)       (None, None, 1024)   6295552     bi_lstm_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 2348)   0           bi_lstm_1[0][0]                  \n",
      "                                                                 bi_lstm_0[0][0]                  \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attlayer (Attention)            [(None, 2348), (None 300800      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2348)         0           attlayer[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Dense)                 (None, 2)            4698        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Concatenate)           (None, None)         0           softmax[0][0]                    \n",
      "                                                                 attlayer[0][1]                   \n",
      "==================================================================================================\n",
      "Total params: 17,264,298\n",
      "Trainable params: 17,264,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 10:22:09,246 - sequence.py[line:346] - INFO: Training with 96 non-empty buckets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mointor training process using f1 score\n",
      "using Early Stopping\n",
      "using Reduce LR On Plateau\n",
      "tracking loss history and metrics\n",
      "Epoch 1/25\n",
      "1384/1384 [==============================] - 135s 98ms/step - loss: 0.9319 - acc: 0.8271 - val_loss: 0.8424 - val_acc: 0.8466\n",
      "0 - f1: 87.52\n",
      "1 - f1: 86.96\n",
      "Epoch 2/25\n",
      " 330/1384 [======>.......................] - ETA: 1:34 - loss: 0.8630 - acc: 0.8386"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-4227cf0e0c11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_mode'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'fold'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtext_classifier_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bucket'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtext_classifier_new\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\nlp_toolkit\\nlp_toolkit\\classifier.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0mreturn_att\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         return self.model_trainer.train(\n\u001b[1;32m--> 133\u001b[1;33m             x, y, self.transformer, self.seq_type, return_att)\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_attention\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\nlp_toolkit\\nlp_toolkit\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x, y, transformer, seq_type, return_attention, use_crf)\u001b[0m\n\u001b[0;32m    228\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                     \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m                     validation_data=valid_batches)\n\u001b[0m\u001b[0;32m    231\u001b[0m                 \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config['train']['train_mode'] = 'fold'\n",
    "text_classifier_new = Classifier(model_name=model_name, transformer=transformer, seq_type='bucket', config=config)\n",
    "text_classifier_new.train(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict New Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 10:26:01,727 - data.py[line:98] - INFO: transformer loaded\n",
      "2018-11-25 10:26:01,916 - data.py[line:136] - INFO: data loaded\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(mode='predict', fname='../data/company_pro_con_predict.txt',\n",
    "                  task_type='classification',\n",
    "                  tran_fname='models/bi_lstm_att_201811242328/transformer.h5')\n",
    "x_seq = dataset.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "text_classifier = Classifier('bi_lstm_att', dataset.transformer)\n",
    "text_classifier.load(weight_fname='models/bi_lstm_att_201811242328/model_weights_07_0.8364_0.9436.h5',\n",
    "                     para_fname='models/bi_lstm_att_201811242328/model_parameters.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 10:32:11,782 - classifier.py[line:144] - INFO: predict 94635 samples used 169.0s\n"
     ]
    }
   ],
   "source": [
    "y_pred, attention = text_classifier.predict(x_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "currently only support for bi_lstm_att model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len = x_seq['length']\n",
    "attention_true = [attention[i][:x_len[i]] for i in range(len(x_len))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_toolkit import visualization as vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <span style=\"background-color: #FFFEFE\">老板</span> <span style=\"background-color: #FFFEFE\">人</span> <span style=\"background-color: #FFB9B9\">很好</span> <span style=\"background-color: #FFFCFC\">老</span> <span style=\"background-color: #FFFEFE\">员工</span> <span style=\"background-color: #FFFEFE\">会</span> <span style=\"background-color: #FFF9F9\">各种</span> <span style=\"background-color: #FFB5B5\">教</span> <span style=\"background-color: #FFCBCB\">你</span> <span style=\"background-color: #FFFEFE\">东西</span> <span style=\"background-color: #FFFEFE\">，</span> <span style=\"background-color: #FFFEFE\">而且</span> <span style=\"background-color: #FFEFEF\">不会</span> <span style=\"background-color: #FFFEFE\">有所</span> <span style=\"background-color: #FFFEFE\">保留</span> <span style=\"background-color: #FFFEFE\">薪水</span> <span style=\"background-color: #FFFEFE\">在</span> <span style=\"background-color: #FFFEFE\">大连</span> <span style=\"background-color: #FFFEFE\">还</span> <span style=\"background-color: #FFFCFC\">算</span> <span style=\"background-color: #FFDDDD\">可以</span><br><br>\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs.att2html(dataset.texts[2], attention_true[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFEFE\">老板</span> <span style=\"background-color: #FFFEFE\">人</span> <span style=\"background-color: #FFD6D6\">很好</span> <span style=\"background-color: #FFFCFC\">老</span> <span style=\"background-color: #FFFEFE\">员工</span> <span style=\"background-color: #FFFEFE\">会</span> <span style=\"background-color: #FFFBFB\">各种</span> <span style=\"background-color: #FFD3D3\">教</span> <span style=\"background-color: #FFD7D7\">你</span> <span style=\"background-color: #FFECEC\">东西</span> <span style=\"background-color: #FFFCFC\">，</span> <span style=\"background-color: #FFFEFE\">而且</span> <span style=\"background-color: #FFD4D4\">不会</span> <span style=\"background-color: #FFFEFE\">有所</span> <span style=\"background-color: #FFFEFE\">保留</span> <span style=\"background-color: #FFFEFE\">薪水</span> <span style=\"background-color: #FFFEFE\">在</span> <span style=\"background-color: #FFFEFE\">大连</span> <span style=\"background-color: #FFFEFE\">还</span> <span style=\"background-color: #FFEDED\">算</span> <span style=\"background-color: #FFD5D5\">可以</span><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or you can write all results to html file and open it in a browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.attention_visualization(dataset.texts, attention_true, x_len, output_fname='result.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
