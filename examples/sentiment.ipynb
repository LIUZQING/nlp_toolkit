{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimet Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo tutorial for how to use nlp_toolkit to train classification model and predict new samples. The task we choose is sentiment binary classification.\n",
    "\n",
    "The dataset is crawled from Kanzhun.com and Dajie.com, which is about company pros and cons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available models:\n",
    "    1. Bi-LSTM Attention\n",
    "    2. Multi Head Self Attention\n",
    "    3. TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/wangyilei/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from nlp_toolkit import Dataset, Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "config = yaml.load(open('../nlp_toolkit/config.yaml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 20:10:20,439 - data.py[line:89] - INFO: data loaded\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(fname='../data/company_pro_con.txt', task_type='classification', mode='train', segment=False, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "进去 前 许诺 的 工资 给 的 高 0\n",
      "校园 环境 优美 ， 美女 很多 ， 适合 居住 ， 食堂 饭菜 便宜 ， 操场 好 ， 可以 天天 运动 0\n",
      "老板 人 很好 老 员工 会 各种 教 你 东西 ， 而且 不会 有所 保留 薪水 在 大连 还 算 可以 0\n",
      "人员 比较 多 ， 复杂   办公室 容易 形成 拉帮结派 不利于 企业 发展 1\n",
      "出差 太多 了 。 在 现场 开发 很苦 逼 。 1\n",
      "公司 目前 地理位置 不 太 理想 ， 离 城市 中心 较 远点 。 1\n",
      "公司 的 技术 水平 国内 顶尖 ， 十几 年 的 资历 ， 制作 的 作品 几乎 都 是 精品 ， 参与 过 很多 知名 项目 。 0\n",
      "工作 流程 复杂     个人 上升 空间 有限     新产品 的 创新 能力 有限   组织 架构 稍 显 臃肿 1\n",
      "无偿 加班 ， 加班 多 ， 没 加班费 ， 压力 很大 1\n",
      "环境 比较 轻松 ， 跟 项目 走 ， 能 学 不少 专业 知识 ， 经验 很 重要 0\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(dataset.texts[0:10], dataset.labels[0:10]):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 20:10:31,165 - data.py[line:116] - INFO: texts and labels transformed to number index\n",
      "2018-11-10 20:10:35,576 - utilities.py[line:73] - INFO: OOV rate: 0.00 %\n",
      "2018-11-10 20:10:35,593 - data.py[line:122] - INFO: Loaded Pre_trained Embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94635, 100) (94635, 2)\n"
     ]
    }
   ],
   "source": [
    "# if we want to use pre_trained embeddings, we need a gensim-format embedding file\n",
    "x, y, config = dataset.transform()\n",
    "print(x['word'].shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if your want to see the vocab and label index mapping dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.transformer._word_vocab._token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.transformer._label_vocab._token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = dataset.transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='bi_lstm_att'\n",
    "# if you want to get attention weights during prediction, please set return_attention=True\n",
    "config[model_name]['return_att'] = True\n",
    "text_classifier = Classifier(model_name=model_name, transformer=transformer, seq_type='bucket', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 19:08:34,366 - trainer.py[line:113] - INFO: bi_lstm_att model structure...\n",
      "2018-11-10 19:08:34,385 - trainer.py[line:123] - INFO: train/valid set: 75708/18927\n",
      "2018-11-10 19:08:34,386 - trainer.py[line:80] - INFO: use bucket sequence to speed up model training\n",
      "2018-11-10 19:08:34,388 - sequence.py[line:300] - INFO: Training with 99 non-empty buckets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word (InputLayer)               (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 300)    7333200     word[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 300)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embed_drop (SpatialDropout1D)   (None, None, 300)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_0 (Bidirectional)       (None, None, 1024)   3330048     embed_drop[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_1 (Bidirectional)       (None, None, 1024)   6295552     bi_lstm_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 2348)   0           bi_lstm_1[0][0]                  \n",
      "                                                                 bi_lstm_0[0][0]                  \n",
      "                                                                 embed_drop[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attlayer (Attention)            [(None, 2348), (None 300800      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2348)         0           attlayer[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Dense)                 (None, 2)            4698        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Concatenate)           (None, None)         0           softmax[0][0]                    \n",
      "                                                                 attlayer[0][1]                   \n",
      "==================================================================================================\n",
      "Total params: 17,264,298\n",
      "Trainable params: 17,264,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 19:08:34,725 - sequence.py[line:300] - INFO: Training with 98 non-empty buckets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mointor training process using f1 score\n",
      "Successfully made a directory: models/bi_lstm_att_201811101908\n",
      "using Early Stopping\n",
      "using Reduce LR On Plateau\n",
      "tracking loss history and metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 19:08:35,100 - trainer.py[line:154] - INFO: saving model parameters and transformer to models/bi_lstm_att_201811101908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model hyperparameters:\n",
      " {'nb_classes': 2, 'nb_tokens': 24444, 'maxlen': None, 'embedding_dim': 300, 'rnn_size': 512, 'attention_dim': 128, 'embed_dropout_rate': 0.25, 'final_dropout_rate': 0.5, 'return_attention': True}\n",
      "Epoch 1/25\n",
      "1236/1236 [==============================] - 156s 126ms/step - loss: 0.4329 - acc: 0.7741 - val_loss: 0.2684 - val_acc: 0.8823\n",
      "0 - f1: 93.41\n",
      "1 - f1: 92.46\n",
      "\n",
      "Epoch 00001: f1 improved from -inf to 0.92963, saving model to models/bi_lstm_att_201811101908/model_weights_01_0.8823_0.9296.h5\n",
      "Epoch 2/25\n",
      "1236/1236 [==============================] - 147s 119ms/step - loss: 0.2245 - acc: 0.8851 - val_loss: 0.2224 - val_acc: 0.8846\n",
      "0 - f1: 94.31\n",
      "1 - f1: 93.63\n",
      "\n",
      "Epoch 00002: f1 improved from 0.92963 to 0.93985, saving model to models/bi_lstm_att_201811101908/model_weights_02_0.8846_0.9398.h5\n",
      "Epoch 3/25\n",
      "1236/1236 [==============================] - 149s 120ms/step - loss: 0.1686 - acc: 0.9009 - val_loss: 0.2189 - val_acc: 0.8746\n",
      "0 - f1: 94.43\n",
      "1 - f1: 93.93\n",
      "\n",
      "Epoch 00003: f1 improved from 0.93985 to 0.94196, saving model to models/bi_lstm_att_201811101908/model_weights_03_0.8746_0.9420.h5\n",
      "Epoch 4/25\n",
      "1236/1236 [==============================] - 150s 121ms/step - loss: 0.1542 - acc: 0.8971 - val_loss: 0.2234 - val_acc: 0.8797\n",
      "0 - f1: 94.39\n",
      "1 - f1: 93.73\n",
      "\n",
      "Epoch 00004: f1 did not improve from 0.94196\n",
      "Epoch 5/25\n",
      "1236/1236 [==============================] - 150s 121ms/step - loss: 0.1479 - acc: 0.9011 - val_loss: 0.2219 - val_acc: 0.8796\n",
      "0 - f1: 94.46\n",
      "1 - f1: 93.83\n",
      "\n",
      "Epoch 00005: f1 did not improve from 0.94196\n",
      "Epoch 6/25\n",
      "1236/1236 [==============================] - 150s 121ms/step - loss: 0.1472 - acc: 0.9006 - val_loss: 0.2214 - val_acc: 0.8785\n",
      "0 - f1: 94.48\n",
      "1 - f1: 93.88\n",
      "\n",
      "Epoch 00006: f1 did not improve from 0.94196\n",
      "best f1: 0.94\n"
     ]
    }
   ],
   "source": [
    "trained_model = text_classifier.train(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 20:13:01,319 - trainer.py[line:169] - INFO: 10-fold starts!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------ fold 0------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 20:13:04,139 - trainer.py[line:180] - INFO: bi_lstm_att model structure...\n",
      "2018-11-10 20:13:04,271 - trainer.py[line:80] - INFO: use bucket sequence to speed up model training\n",
      "2018-11-10 20:13:04,274 - sequence.py[line:300] - INFO: Training with 99 non-empty buckets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word (InputLayer)               (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 300)    7333200     word[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, 300)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embed_drop (SpatialDropout1D)   (None, None, 300)    0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_0 (Bidirectional)       (None, None, 1024)   3330048     embed_drop[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_1 (Bidirectional)       (None, None, 1024)   6295552     bi_lstm_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 2348)   0           bi_lstm_1[0][0]                  \n",
      "                                                                 bi_lstm_0[0][0]                  \n",
      "                                                                 embed_drop[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attlayer (Attention)            [(None, 2348), (None 300800      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2348)         0           attlayer[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Dense)                 (None, 2)            4698        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Concatenate)           (None, None)         0           softmax[0][0]                    \n",
      "                                                                 attlayer[0][1]                   \n",
      "==================================================================================================\n",
      "Total params: 17,264,298\n",
      "Trainable params: 17,264,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 20:13:04,632 - sequence.py[line:300] - INFO: Training with 96 non-empty buckets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mointor training process using f1 score\n",
      "using Early Stopping\n",
      "using Reduce LR On Plateau\n",
      "tracking loss history and metrics\n",
      "Epoch 1/25\n",
      "1384/1384 [==============================] - 161s 116ms/step - loss: 0.3665 - acc: 0.7794 - val_loss: 0.2241 - val_acc: 0.8710\n",
      "0 - f1: 94.37\n",
      "1 - f1: 93.73\n",
      "Epoch 2/25\n",
      "1384/1384 [==============================] - 156s 113ms/step - loss: 0.2036 - acc: 0.8715 - val_loss: 0.1988 - val_acc: 0.9033\n",
      "0 - f1: 94.72\n",
      "1 - f1: 94.30\n",
      "Epoch 3/25\n",
      "1384/1384 [==============================] - 155s 112ms/step - loss: 0.1543 - acc: 0.9002 - val_loss: 0.1946 - val_acc: 0.8634\n",
      "0 - f1: 94.95\n",
      "1 - f1: 94.38\n",
      "Epoch 4/25\n",
      "1384/1384 [==============================] - 156s 113ms/step - loss: 0.1374 - acc: 0.8895 - val_loss: 0.1953 - val_acc: 0.8693\n",
      "0 - f1: 94.95\n",
      "1 - f1: 94.42\n",
      "Epoch 5/25\n",
      "1384/1384 [==============================] - 156s 112ms/step - loss: 0.1332 - acc: 0.8890 - val_loss: 0.1959 - val_acc: 0.8664\n",
      "0 - f1: 94.99\n",
      "1 - f1: 94.48\n",
      "Epoch 6/25\n",
      "1384/1384 [==============================] - 157s 113ms/step - loss: 0.1311 - acc: 0.8874 - val_loss: 0.1968 - val_acc: 0.8664\n",
      "0 - f1: 95.05\n",
      "1 - f1: 94.51\n",
      "Epoch 7/25\n",
      "1384/1384 [==============================] - 156s 113ms/step - loss: 0.1316 - acc: 0.8854 - val_loss: 0.1966 - val_acc: 0.8639\n",
      "0 - f1: 95.09\n",
      "1 - f1: 94.56\n",
      "Epoch 8/25\n",
      "1384/1384 [==============================] - 156s 113ms/step - loss: 0.1287 - acc: 0.8825 - val_loss: 0.1979 - val_acc: 0.8622\n",
      "0 - f1: 95.08\n",
      "1 - f1: 94.54\n",
      "Epoch 9/25\n",
      "1384/1384 [==============================] - 156s 113ms/step - loss: 0.1284 - acc: 0.8807 - val_loss: 0.1969 - val_acc: 0.8614\n",
      "0 - f1: 95.03\n",
      "1 - f1: 94.48\n",
      "Epoch 10/25\n",
      "1384/1384 [==============================] - 157s 113ms/step - loss: 0.1270 - acc: 0.8796 - val_loss: 0.1975 - val_acc: 0.8600\n",
      "0 - f1: 95.07\n",
      "1 - f1: 94.54\n",
      "\n",
      "------------------------ fold 1------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 20:40:50,748 - trainer.py[line:80] - INFO: use bucket sequence to speed up model training\n",
      "2018-11-10 20:40:50,751 - sequence.py[line:300] - INFO: Training with 99 non-empty buckets\n",
      "2018-11-10 20:40:51,101 - sequence.py[line:300] - INFO: Training with 97 non-empty buckets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mointor training process using f1 score\n",
      "using Early Stopping\n",
      "using Reduce LR On Plateau\n",
      "tracking loss history and metrics\n",
      "Epoch 1/25\n",
      "1383/1383 [==============================] - 162s 117ms/step - loss: 0.3200 - acc: 0.6720 - val_loss: 0.2858 - val_acc: 0.7245\n",
      "0 - f1: 92.48\n",
      "1 - f1: 92.05\n",
      "Epoch 2/25\n",
      "1383/1383 [==============================] - 154s 112ms/step - loss: 0.2678 - acc: 0.7517 - val_loss: 0.2643 - val_acc: 0.7757\n",
      "0 - f1: 92.92\n",
      "1 - f1: 92.58\n",
      "Epoch 3/25\n",
      "1383/1383 [==============================] - 167s 121ms/step - loss: 0.2481 - acc: 0.7852 - val_loss: 0.2472 - val_acc: 0.7898\n",
      "0 - f1: 93.33\n",
      "1 - f1: 93.01\n",
      "Epoch 4/25\n",
      "1383/1383 [==============================] - 175s 127ms/step - loss: 0.2338 - acc: 0.7989 - val_loss: 0.2351 - val_acc: 0.8081\n",
      "0 - f1: 93.71\n",
      "1 - f1: 93.32\n",
      "Epoch 5/25\n",
      "1383/1383 [==============================] - 177s 128ms/step - loss: 0.2234 - acc: 0.8156 - val_loss: 0.2261 - val_acc: 0.8145\n",
      "0 - f1: 93.94\n",
      "1 - f1: 93.62\n",
      "Epoch 6/25\n",
      "1383/1383 [==============================] - 175s 127ms/step - loss: 0.2147 - acc: 0.8220 - val_loss: 0.2173 - val_acc: 0.8192\n",
      "0 - f1: 94.16\n",
      "1 - f1: 93.76\n",
      "Epoch 7/25\n",
      "1383/1383 [==============================] - 175s 126ms/step - loss: 0.2055 - acc: 0.8296 - val_loss: 0.2105 - val_acc: 0.8307\n",
      "0 - f1: 94.24\n",
      "1 - f1: 93.92\n",
      "Epoch 8/25\n",
      "1383/1383 [==============================] - 155s 112ms/step - loss: 0.1977 - acc: 0.8408 - val_loss: 0.2041 - val_acc: 0.8442\n",
      "0 - f1: 94.60\n",
      "1 - f1: 94.25\n",
      "Epoch 9/25\n",
      "1383/1383 [==============================] - 154s 111ms/step - loss: 0.1927 - acc: 0.8500 - val_loss: 0.1980 - val_acc: 0.8507\n",
      "0 - f1: 94.78\n",
      "1 - f1: 94.40\n",
      "Epoch 10/25\n",
      "1383/1383 [==============================] - 155s 112ms/step - loss: 0.1871 - acc: 0.8580 - val_loss: 0.1945 - val_acc: 0.8539\n",
      "0 - f1: 94.77\n",
      "1 - f1: 94.45\n",
      "Epoch 11/25\n",
      "1383/1383 [==============================] - 178s 128ms/step - loss: 0.1832 - acc: 0.8618 - val_loss: 0.1913 - val_acc: 0.8615\n",
      "0 - f1: 94.89\n",
      "1 - f1: 94.60\n",
      "Epoch 12/25\n",
      "1383/1383 [==============================] - 158s 115ms/step - loss: 0.1782 - acc: 0.8668 - val_loss: 0.1865 - val_acc: 0.8642\n",
      "0 - f1: 95.10\n",
      "1 - f1: 94.76\n",
      "Epoch 13/25\n",
      "1383/1383 [==============================] - 159s 115ms/step - loss: 0.1750 - acc: 0.8720 - val_loss: 0.1852 - val_acc: 0.8695\n",
      "0 - f1: 95.18\n",
      "1 - f1: 94.88\n",
      "Epoch 14/25\n",
      "1383/1383 [==============================] - 171s 124ms/step - loss: 0.1734 - acc: 0.8726 - val_loss: 0.1830 - val_acc: 0.8695\n",
      "0 - f1: 95.17\n",
      "1 - f1: 94.81\n",
      "Epoch 15/25\n",
      " 398/1383 [=======>......................] - ETA: 1:52 - loss: 0.1703 - acc: 0.8848"
     ]
    }
   ],
   "source": [
    "config['train']['train_mode'] = 'fold'\n",
    "text_classifier = Classifier(model_name=model_name, transformer=transformer, seq_type='bucket', config=config)\n",
    "text_classifier.train(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict New Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 19:44:27,999 - data.py[line:73] - INFO: transformer loaded\n",
      "2018-11-10 19:44:28,283 - data.py[line:89] - INFO: data loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data transformer loaded\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset('../data/company_pro_con_predict.txt',\n",
    "                  task_type='classification', mode='predict',\n",
    "                  tran_fname='models/bi_lstm_att_201811101908/transformer.h5',\n",
    "                  segment=False)\n",
    "x_seq = dataset.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "text_classifier = Classifier('bi_lstm_att', dataset.transformer)\n",
    "text_classifier.load(weight_fname='models/bi_lstm_att_201811101908/model_weights_03_0.8746_0.9420.h5',\n",
    "                     para_fname='models/bi_lstm_att_201811101908/model_parameters.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-10 19:50:14,200 - classifier.py[line:121] - INFO: predict 94635 samples used 193.8s\n"
     ]
    }
   ],
   "source": [
    "y_pred, attention = text_classifier.predict(x_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len = x_seq['length']\n",
    "attention_true = [attention[i][:x_len[i]] for i in range(len(x_len))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_toolkit import visualization as vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <span style=\"background-color: #FFFEFE\">老板</span> <span style=\"background-color: #FFFEFE\">人</span> <span style=\"background-color: #FFD6D6\">很好</span> <span style=\"background-color: #FFFCFC\">老</span> <span style=\"background-color: #FFFEFE\">员工</span> <span style=\"background-color: #FFFEFE\">会</span> <span style=\"background-color: #FFFBFB\">各种</span> <span style=\"background-color: #FFD3D3\">教</span> <span style=\"background-color: #FFD7D7\">你</span> <span style=\"background-color: #FFECEC\">东西</span> <span style=\"background-color: #FFFCFC\">，</span> <span style=\"background-color: #FFFEFE\">而且</span> <span style=\"background-color: #FFD4D4\">不会</span> <span style=\"background-color: #FFFEFE\">有所</span> <span style=\"background-color: #FFFEFE\">保留</span> <span style=\"background-color: #FFFEFE\">薪水</span> <span style=\"background-color: #FFFEFE\">在</span> <span style=\"background-color: #FFFEFE\">大连</span> <span style=\"background-color: #FFFEFE\">还</span> <span style=\"background-color: #FFEDED\">算</span> <span style=\"background-color: #FFD5D5\">可以</span><br><br>\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs.mk_html(dataset.texts[2].split(), attention_true[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFEFE\">老板</span> <span style=\"background-color: #FFFEFE\">人</span> <span style=\"background-color: #FFD6D6\">很好</span> <span style=\"background-color: #FFFCFC\">老</span> <span style=\"background-color: #FFFEFE\">员工</span> <span style=\"background-color: #FFFEFE\">会</span> <span style=\"background-color: #FFFBFB\">各种</span> <span style=\"background-color: #FFD3D3\">教</span> <span style=\"background-color: #FFD7D7\">你</span> <span style=\"background-color: #FFECEC\">东西</span> <span style=\"background-color: #FFFCFC\">，</span> <span style=\"background-color: #FFFEFE\">而且</span> <span style=\"background-color: #FFD4D4\">不会</span> <span style=\"background-color: #FFFEFE\">有所</span> <span style=\"background-color: #FFFEFE\">保留</span> <span style=\"background-color: #FFFEFE\">薪水</span> <span style=\"background-color: #FFFEFE\">在</span> <span style=\"background-color: #FFFEFE\">大连</span> <span style=\"background-color: #FFFEFE\">还</span> <span style=\"background-color: #FFEDED\">算</span> <span style=\"background-color: #FFD5D5\">可以</span><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or you can write all results to html file and open it in a browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.attention_visualization(dataset.texts, attention_true, x_len, output_fname='result.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
